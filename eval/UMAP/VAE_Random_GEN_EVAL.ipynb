{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "run_name = \"noble-field-7\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run = wandb.init()\n",
    "links = {\n",
    "    \"GOOD_AVERAGE_glamorous-sweep-62\": \"mmil_vae_g2d/voice_distribution_and_genre_distribution_imbalance/model_epoch_100:v283\",\n",
    "    \"GOOD_azure-sweep-54\": \"mmil_vae_g2d/voice_distribution_and_genre_distribution_imbalance/model_epoch_100:v279\",\n",
    "    \"GOOD_apricot-sweep-17\": \"mmil_vae_g2d/voice_distribution_and_genre_distribution_imbalance/model_epoch_100:v242\",\n",
    "    \"GOOD_hearty-sweep-60\": \"mmil_vae_g2d/voice_distribution_and_genre_distribution_imbalance/model_epoch_100:v280\",\n",
    "    \"GOOD_worldly-sweep-22\": \"mmil_vae_g2d/voice_distribution_and_genre_distribution_imbalance/model_epoch_100:v245\",\n",
    "    \"GOOD_legendary-sweep-5\": \"mmil_vae_g2d/voice_distribution_and_genre_distribution_imbalance/model_epoch_100:v230\",\n",
    "    \"drawn_river_6\": \"mmil_vae_g2d/beta_annealing_study/model_epoch_100:v2\",\n",
    "    \"worldly-firebrand-5\": \"mmil_vae_g2d/beta_annealing_study/model_epoch_100:v1\",\n",
    "    \"noble-field-7\": \"mmil_vae_g2d/beta_annealing_study/model_epoch_100:v3\",\n",
    "    \"young-violet-12\": \"mmil_vae_g2d/beta_annealing_study/model_epoch_700:v0\",\n",
    "    \"kind-gorge-14\": \"mmil_vae_g2d/beta_annealing_study/model_epoch_500:v1\"\n",
    "    \n",
    "}\n",
    "artifact = run.use_artifact(links[run_name], type='model')\n",
    "artifact_dir = artifact.download()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers import load_variational_mgt_model\n",
    "from model import GrooveTransformerEncoderVAE\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from helpers import load_variational_mgt_model   \n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_variational_mgt_model(os.path.join(artifact_dir, \"100.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from data import load_gmd_hvo_sequences\n",
    "\n",
    "train_set = load_gmd_hvo_sequences(\n",
    "    dataset_setting_json_path = \"../../data/dataset_json_settings/4_4_Beats_gmd.json\", \n",
    "    subset_tag = \"validation\", \n",
    "    force_regenerate=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_sample = train_set[0]\n",
    "groove = torch.tensor([gt_sample.flatten_voices(reduce_dim=True)], dtype=torch.float32)\n",
    "gt_sample.metadata"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu, logvar = model.encode_to_mu_logvar(groove)\n",
    "latent_z = model.reparametrize(mu, logvar)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latent_z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "voice_thresholds = [0.5] * 9\n",
    "voice_max_count_allowed = [32] * 9\n",
    "h, v, o = model.sample(latent_z=latent_z,\n",
    "                       voice_thresholds=voice_thresholds,\n",
    "                       voice_max_count_allowed=voice_max_count_allowed,\n",
    "                       return_concatenated=False,\n",
    "                       sampling_mode=0)\n",
    "print(h, v, o)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "latents = []\n",
    "labels = []\n",
    "metadatas = []\n",
    "use_all_styles = True\n",
    "empty_hvo_seqs = []\n",
    "for gt_sample in train_set:\n",
    "    if (use_all_styles or (gt_sample.metadata[\"style_primary\"] in [\"rock\", \"funk\", \"afrobeat\"])):\n",
    "        empty_hvo_seqs.append(gt_sample.copy_empty())\n",
    "        metadatas.append(gt_sample.metadata)\n",
    "        labels.append(gt_sample.metadata[\"style_primary\"])\n",
    "        groove = torch.zeros((1, 32, 3))\n",
    "        flattened_ = torch.tensor([gt_sample.flatten_voices(reduce_dim=True)], dtype=torch.float32)[:,:32, :]\n",
    "        t_steps = flattened_.shape[1]\n",
    "        groove[:, :t_steps, :] = flattened_\n",
    "        mu, logvar = model.encode_to_mu_logvar(groove)\n",
    "        latent_z = model.reparametrize(mu, logvar)\n",
    "        latents.append(latent_z.detach().cpu().numpy())\n",
    "\n",
    "latents = np.array(latents).squeeze(1)\n",
    "features = np.expand_dims(latents, -1) # we use each dimension of latent_z as a feature\n",
    "feature_labels = [f\"z_{dim}\" for dim in range(features.shape[1])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\"style_primary\": [style for style in labels]}\n",
    "data.update({f\"z_{dim_i}\": latents[:, dim_i] for dim_i in range(features.shape[1])})\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "import IPython.display as ipd\n",
    "import sys\n",
    "# sys.path.insert(0, '/usr/local/bin/fluidsynth')\n",
    "output_notebook()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "voice_thresholds=[0.3]*9\n",
    "voice_thresholds[1] = 0.5\n",
    "voice_thresholds[3] = 0.1\n",
    "\n",
    "# voice_thresholds[-2] = 0.01\n",
    "# voice_thresholds[-3] = 0.01\n",
    "\n",
    "# random_z = [np.random.uniform(df[f\"z_{i}\"].min(), df[f\"z_{i}\"].max()) for i in range(len(latents[0, :]))]\n",
    "# random_z = [np.random.uniform(df[f\"z_{i}\"].min(), df[f\"z_{i}\"].max()) for i in range(len(latents[0, :]))]\n",
    "random_z = [np.random.normal(loc=df[f\"z_{i}\"].mean(), scale=df[f\"z_{i}\"].std()) for i in range(len(latents[0, :]))]\n",
    "\n",
    "\n",
    "hvo = model.sample(latent_z=torch.tensor(random_z, dtype=torch.float32), \n",
    "                   voice_thresholds=voice_thresholds,\n",
    "                   voice_max_count_allowed=[32]*9,\n",
    "                   return_concatenated=True,\n",
    "                   sampling_mode=0)\n",
    "hvo_seq_ = empty_hvo_seqs[0]\n",
    "hvo_seq_.hvo = hvo.detach().cpu().numpy()[0]\n",
    "\n",
    "if (hvo_seq_.get_number_of_active_voices()>0):\n",
    "    # draw and synthesize\n",
    "    show(hvo_seq_.piano_roll())\n",
    "    audio = hvo_seq_.synthesize(sf_path=\"../../hvo_sequence/soundfonts/TamaRockSTAR.sf2\")\n",
    "\n",
    "else:\n",
    "    print(\"Empty Score\")\n",
    "    \n",
    "ipd.Audio(audio, rate=44100, autoplay=True) # load a NumPy array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2ff3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrooveTransformer",
   "language": "python",
   "name": "groovetransformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}